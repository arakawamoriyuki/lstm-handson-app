{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMハンズオン\n",
    "\n",
    "はじめに、機械学習とニューラルネットワークについて少し説明します。\n",
    "\n",
    "機械学習はトレーニングデータと答えさえあればロジックを考えなくても最適な答えを出してくれる素敵な数学モデルです。(教師なし学習など、一部例外はあります)\n",
    "\n",
    "ニューラルネットワークは機械学習の一種で、wikiから抜粋すると、脳機能に見られるいくつかの特性を計算機上のシミュレーションによって表現することを目指した数学モデルです。層を用いた学習方法が特徴で、層を深くした(その分計算コストをかなり増える)モデルをディープニューラルネットワークと言います。\n",
    "\n",
    "<img src=\"volume/images/ml_venn.png\" width=\"400\">\n",
    "\n",
    "RNNはニューラルネットワークの一種で、文章など連続的な情報を利用できる、時系列データに威力を発揮するモデルです。\n",
    "\n",
    "株価などの数値データも時系列データですし、文章も単語の流れとして捉えて処理することができます。そして以前に計算された情報を覚えるための記憶力を持っていることが特徴です。\n",
    "\n",
    "LSTMはRNNの一種で、RNNを改良した上位互換のモデルで、下記のようなタスクをこなせます。(できることの一部です)\n",
    "\n",
    "- 連続した数値を入力して、数値を出力\n",
    "- 画像を入力としてそのキャプション(文字)を出力\n",
    "- 音声を入力として文字を出力\n",
    "\n",
    "[参考](https://deepage.net/deep_learning/2017/05/23/recurrent-neural-networks.html)\n",
    "\n",
    "今回はkeras(ニューラルネットワークのライブラリ)を利用した時系列データの予測を行います。\n",
    "\n",
    "- 規則性のある波を予測 (値1つで予測)\n",
    "- 規則性のあるアイスクリームの売り上げ予測 (売り上げだけではなく気温など複数の値を考慮して予測)\n",
    "- 規則性のないbitcoinの値段を予測 (最終取引価格や最高売り価格などで予測)\n",
    "- おまけでword2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# jupiter上でグラフ表示\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 規則性のある波を予測 (値1つで予測)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-1 データセットの作成\n",
    "\n",
    "学習する上でまずデータセットが必要になります。データセットは、入力と出力、今回は時系列データとその後の値が必要になります。\n",
    "\n",
    "まずは学習に使うデータを作ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sin関数で波生成\n",
    "def sin(length=100, curve=4):\n",
    "    x = np.arange(0, length)\n",
    "    return np.sin((curve * np.pi) * x / length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sin())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "このままでは単純にsinを計算すれば求められるので、ある程度ノイズを乗せて予測しずらそうなデータにしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ノイズ付きsin波\n",
    "def noised_sin(length=100, curve=4, noise_rate=0.3, low=-1.0, high=1.0):\n",
    "    x = sin(length=length, curve=curve)\n",
    "    noise = noise_rate * np.random.uniform(low=low, high=high, size=len(x))\n",
    "    return x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(noised_sin())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ノイズを乗せれたので今度はデータを時系列データとして整形します。\n",
    "\n",
    "時系列データの学習では数個の連続したデータを1セットとして用意する必要があります。\n",
    "\n",
    "以下の関数で配列をstep個に分けたデータセットと答え(次の値)を作れます。\n",
    "\n",
    "戻り値はnumpyで作られたarray like object(配列っぽいオブジェクト)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reccurent_dataset(vector_data, step=25):\n",
    "    data, target = [], []\n",
    "    for i in range(len(vector_data) - step):\n",
    "        data.append(vector_data[i:(i + step)])\n",
    "        target.append(vector_data[i + step])\n",
    "    reshape_data = np.array(data).reshape(len(data), step, 1)\n",
    "    reshape_label = np.array(target).reshape(len(data), 1)\n",
    "    return reshape_data, reshape_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "適当な値を入れてみて、データの形状と中身の確認をしてみましょう。\n",
    "\n",
    "shapeプロパティで形状を確認できます。多次元配列版 lengthのようなものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_label = create_reccurent_dataset([1,2,3,4,5,6,7,8,9], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3個の連続した値を6つ持った配列になりました。\n",
    "\n",
    "具体的にトレーニングデータは、(学習データ総数, 1学習の時系列数(ステップ数), 特徴量)で整形する必要があります。\n",
    "\n",
    "3次元配列で出てきますが、他にも気温、湿度、曜日など予測に役立ちそうなデータを入れられます。(今回は1つのデータなので三番目は1になってます)\n",
    "\n",
    "次に中身も確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "連続した値を3つずつ分割して計6個の学習用データが入っています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "次に答えラベル(次に来るべき値)も確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "トレーニングデータ6個に対して6つの答えラベルが用意できています。\n",
    "\n",
    "中身も確認してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1,2,3]の答えとして4、[2,3,4]の答えとして5...というようにトレーニングデータに対しての答えも用意できました。\n",
    "\n",
    "では、実際にノイズつきデータをデータセットとして整形してみましょう。\n",
    "\n",
    "length(データ個数)やnoise_rate(ノイズの度合い)をあげたり、curve(カーブ数)を変えてもらっても構いません！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ノイズ付きsin波予測\n",
    "data = noised_sin(length=100, curve=4, noise_rate=0.3)\n",
    "train_data, train_label = create_reccurent_dataset(data, step=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "パラメータ変えていなければ25個の連続したデータが75個できたと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-2 モデル構築\n",
    "\n",
    "時系列(長短期記憶)学習モデルは今回使用するLSTM以外にもたくさんあります。QRNNが新しく、良さそう。\n",
    "\n",
    "今回はLSTMを使います。\n",
    "\n",
    "- RNN\n",
    "- LSTM\n",
    "- [QRNN](https://qiita.com/icoxfog417/items/d77912e10a7c60ae680e)\n",
    "\n",
    "tensorflow内蔵のkerasを利用します。(kerasはtensorflowのラッパーです)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Activation, LSTM\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "step_count = 1つの学習データのStep数(今回は25)\n",
    "\n",
    "feature_count = 特徴量 今回の学習するデータは1個のみ\n",
    "\n",
    "hidden_unit_count = 中間層の数 ([公式](https://keras.io/ja/layers/recurrent/#lstm)の説明は出力の次元数ですが、LSTMの次に全層結合を挟むので中間層の数になります)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_count = 25\n",
    "feature_count = 1\n",
    "hidden_unit_count = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(\n",
    "        hidden_unit_count,\n",
    "        batch_input_shape=(None, step_count, feature_count), # 入力 (データ数(未知数なのでNone), step数, 特徴数)\n",
    "        return_sequences=False\n",
    "    )\n",
    ")\n",
    "model.add(Dense(feature_count)) # LSTMからでた300個のノードを1つの値にまとめます。\n",
    "model.add(Activation('linear')) # 活性化関数 他にもsoftmaxとかsigmoidとかreluとか\n",
    "optimizer = Adam(lr=0.001) # 最適化関数にadamを利用し、学習率0.001でトレーニング\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# jupiter上でmodelの図をみれたり画像に保存できたりするがtensorflow1.4では動かなかった。\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from tensorflow.python.keras._impl.keras.utils.vis_utils import model_to_dot \n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# from tensorflow.python.keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "最後の出力がどうなるのか確認します。\n",
    "\n",
    "(何個渡されるか現時点でわからないのでNone, 答え(出力)数) になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-3 トレーニング\n",
    "\n",
    "早速トレーニングしてみましょう。\n",
    "\n",
    "トレーニング中、callbackできます。\n",
    "\n",
    "今回はcallbackに過学習(Over fitting)抑制や十分学習できた際に終了する為にEarlyStoppingを仕込みます。\n",
    "\n",
    "学習回数(epochs)を考える手間を減らすことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 30~50epochsくらいで十分学習できたと判断して終了するはず\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    patience=20 # 値が良い方向に動かなくなった回数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "精度を示す指標の計算方法もありますが、今回は学習しきった度合いであるlossを参考にしましょう。\n",
    "\n",
    "低いほど良い学習が行われています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# トレーニング\n",
    "model.fit(\n",
    "    train_data, train_label,\n",
    "    batch_size=300, # 指定個数ごとに重みが更新される\n",
    "    epochs=100, # 指定回数学習\n",
    "    validation_split=0.1, # 指定割合のデータをテスト用に利用する\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-4 予測\n",
    "\n",
    "まずはトレーニングしたデータに対して予測してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_dataは75個なので75個の予測値が帰ってきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "データセットと予測した値を並べて正しそうか確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0, len(predictions)), predictions, color='r', label='predictions')\n",
    "plt.plot(range(0, len(train_label)), train_label, color='b', label='training_data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "これだけでは面白くないので、未来の値を予測します。\n",
    "\n",
    "トレーニングデータの最後からstep分抜き出して予測\n",
    "\n",
    "その予測結果も含め、最後からstep分抜き出して予測...\n",
    "\n",
    "を繰り返して予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latest_data = train_data[len(train_data)-1]\n",
    "results = np.empty((0, 0))\n",
    "for _ in range(50):\n",
    "    # 予測\n",
    "    test_data = np.reshape(latest_data, (1, 25, 1))\n",
    "    batch_predict = model.predict(test_data)\n",
    "\n",
    "    # 結果蓄積\n",
    "    results = np.append(results, batch_predict)\n",
    "\n",
    "    # 次のデータをセット\n",
    "    latest_data = np.delete(latest_data, 0)\n",
    "    latest_data = np.append(latest_data, batch_predict)\n",
    "\n",
    "# (50)から(50,1)に形状を変える\n",
    "results = np.reshape(results, (results.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0, len(predictions)), predictions, color='r', label='predictions')\n",
    "plt.plot(range(0, len(train_label)), train_label, color='b', label='training_data')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results, color='g', label='future_predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 アイスクリームの売り上げを予測 (売上だけではなく気温なども考慮した予測)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2-1 データセットの作成\n",
    "\n",
    "先ほどは1つの値だけで予測していましたが、今度は売り上げや気温など、予測に影響しそうな要素も含めて学習させます。\n",
    "\n",
    "実際のデータの方が面白いので東京の気温とアイスクリームの売り上げデータを用意し、利用しやすいように加工しました。\n",
    "\n",
    "[参考データ](https://oku.edu.mie-u.ac.jp/~okumura/stat/160118.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tableのような扱いができるpandas、これまたnumpyのようなarray like objectです。\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "読み込んで表示してみましょう\n",
    "\n",
    "- year 年\n",
    "- month 月\n",
    "- avg_max_temperature 平均最高気温\n",
    "- sum_precipitation_mm 降水量\n",
    "- avg_humidity_per 平均湿度\n",
    "- 25c_days 25度を超えた日数\n",
    "- ice_sale アイスの売り上げ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('volume/datasets/tokyo-weather-and-ice-sales.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(data['avg_max_temperature'], label='avg_max_temperature') # 最大気温平均\n",
    "plt.plot(data['sum_precipitation_mm'], label='sum_precipitation_mm') # 合計降水量\n",
    "plt.plot(data['avg_humidity_per'], label='avg_humidity_per') # 平均湿度\n",
    "plt.plot(data['25c_days'], label='25c_days') # 25度以上の日数\n",
    "plt.plot(data['ice_sale'], label='ice_sale') # アイスの売り上げ(円)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "アイスの売り上げを予測する為に各データを加工します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temps = data['avg_max_temperature'].values.tolist() # 配列化\n",
    "temps = np.array(temps).reshape((len(temps), 1)).astype(float) # 形状変更 (120, 1)\n",
    "print(temps.shape)\n",
    "\n",
    "precs = data['sum_precipitation_mm'].values.tolist()\n",
    "precs = np.array(precs).reshape((len(precs), 1)).astype(float)\n",
    "print(precs.shape)\n",
    "\n",
    "humidities = data['avg_humidity_per'].values.tolist()\n",
    "humidities = np.array(humidities).reshape((len(humidities), 1)).astype(float)\n",
    "print(humidities.shape)\n",
    "\n",
    "up25days = data['25c_days'].values.tolist()\n",
    "up25days = np.array(up25days).reshape((len(up25days), 1)).astype(float)\n",
    "print(up25days.shape)\n",
    "\n",
    "icesales = data['ice_sale'].values.tolist()\n",
    "icesales = np.array(icesales).reshape((len(icesales), 1)).astype(float)\n",
    "print(icesales.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "各データの範囲がバラバラの為、学習に時間が掛かります(数値が高いデータほど重みが大きくなるなどの悪影響もあります)\n",
    "\n",
    "なのでデータ範囲を0~1の範囲に納める標準化/正規化を行います。今回はsklearnのMinMaxScalerを使います。\n",
    "\n",
    "(本当は最終的に出力される値も0~1になってしまうので、データごとに単純に`n/100`とか固定値で割るなどすると後で円や度などの単位に戻しやすくなりますが、今回は割愛します)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "normalized_temps = scaler.fit_transform(temps)\n",
    "normalized_precs = scaler.fit_transform(precs)\n",
    "normalized_humidities = scaler.fit_transform(humidities)\n",
    "normalized_up25days = scaler.fit_transform(up25days)\n",
    "normalized_icesales = scaler.fit_transform(icesales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "データを一部確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_icesales[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "前回は25stepのデータにしました。\n",
    "\n",
    "今回は一年周期で変動する月のデータなので、12の倍数が良さそうな気がします。\n",
    "\n",
    "24とかでも良いですが、データ自体も多いわけではないので今回は12ヶ月(過去1年)をステップとしてトレーニングしてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temps_train_data, temps_train_label = create_reccurent_dataset(normalized_temps, step=12)\n",
    "precs_train_data, precs_train_label = create_reccurent_dataset(normalized_precs, step=12)\n",
    "up25days_train_data, up25days_train_label = create_reccurent_dataset(normalized_up25days, step=12)\n",
    "humidities_train_data, humidities_train_label = create_reccurent_dataset(normalized_humidities, step=12)\n",
    "icesales_train_data, icesales_train_label = create_reccurent_dataset(normalized_icesales, step=12)\n",
    "print(temps_train_data.shape)\n",
    "print(temps_train_label.shape)\n",
    "print(precs_train_data.shape)\n",
    "print(precs_train_label.shape)\n",
    "print(up25days_train_data.shape)\n",
    "print(up25days_train_label.shape)\n",
    "print(humidities_train_data.shape)\n",
    "print(humidities_train_label.shape)\n",
    "print(icesales_train_data.shape)\n",
    "print(icesales_train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "推論する際の形状は(データ個数, ステップ数, 特徴量)です。\n",
    "\n",
    "特徴量は(最大気温平均, 合計降水量, 平均湿度, 25度以上の日数)の5個です。\n",
    "\n",
    "つまり (None, 12 5)を作りたいのでnumpyでデータをマージします。\n",
    "\n",
    "出力をアイスの売り上げだけにすると、未来予測が1個後までしかできなくなるので、5個すべて答えとして用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = np.c_[temps_train_data, precs_train_data, up25days_train_data, humidities_train_data, icesales_train_data]\n",
    "train_label = np.c_[temps_train_label, precs_train_label, up25days_train_label, humidities_train_label, icesales_train_label]\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2-2 モデル構築\n",
    "\n",
    "1のsin波でやったことと同じ内容ですが、入力するデータが12ステップの5個の値、出力が5個の値になります。\n",
    "\n",
    "入力 -> (None, 12, 5) 出力 -> (None, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "step_count = 12\n",
    "feature_count = 5\n",
    "hidden_unit_count = 120\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_unit_count, batch_input_shape=(None, step_count, feature_count)))\n",
    "model.add(Dense(feature_count))\n",
    "model.add(Activation('linear'))\n",
    "optimizer = Adam(lr=0.003)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='auto', patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2-3 トレーニング\n",
    "\n",
    "トレーニングしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_data, train_label,\n",
    "    batch_size=300,\n",
    "    epochs=200,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2-4 予測\n",
    "\n",
    "トレーニングデータと予測結果を合わせて確認してみましょう。\n",
    "\n",
    "多くてグラフが見辛いため特徴ごとに予測できているか確認します。\n",
    "\n",
    "薄い色が正解(トレーニングデータ)、濃い色が予測結果です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 最大気温平均\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,0], color=(1.0, 0.7, 0.7), label='train_label temp')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,0], color=(1.0, 0.0, 0.0), label='predictions temp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 合計降水量\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,1], color=(0.7, 0.7, 1.0), label='train_label prec')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,1], color=(0.0, 0.0, 1.0), label='predictions prec')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 平均湿度\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,2], color=(0.7, 1.0, 1.0), label='train_label humidity')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,2], color=(0.0, 1.0, 1.0), label='predictions humidity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 25度以上の日数\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,3], color=(1.0, 1.0, 0.7), label='train_label 25c_days')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,3], color=(1.0, 1.0, 0.0), label='predictions 25c_days')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# アイスの売り上げ\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,4], color=(0.7, 1.0, 1.0), label='train_label ice_sale')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,4], color=(0.0, 1.0, 1.0), label='predictions ice_sale')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# すべて(見辛い！)\n",
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,0], color=(1.0, 0.7, 0.7), label='train_label temp')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,0], color=(1.0, 0.0, 0.0), label='predictions temp')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,1], color=(0.7, 0.7, 1.0), label='train_label prec')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,1], color=(0.0, 0.0, 1.0), label='predictions prec')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,2], color=(0.7, 1.0, 1.0), label='train_label humidity')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,2], color=(0.0, 1.0, 1.0), label='predictions humidity')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,3], color=(1.0, 1.0, 0.7), label='train_label 25c_days')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,3], color=(1.0, 1.0, 0.0), label='predictions 25c_days')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,4], color=(0.7, 1.0, 1.0), label='train_label ice_sale')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,4], color=(0.0, 1.0, 1.0), label='predictions ice_sale')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5ヶ月後の予測がどうなるかも調べてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latest_data = np.array([train_data[len(train_data)-1]])\n",
    "results = []\n",
    "for _ in range(5):\n",
    "    # 推論\n",
    "    print(latest_data.shape)\n",
    "    batch_predict = model.predict(latest_data)\n",
    "    # 結果蓄積\n",
    "    results.append([batch_predict[0][0], batch_predict[0][1], batch_predict[0][2], batch_predict[0][3], batch_predict[0][4]])\n",
    "    # 次のデータをセット\n",
    "    latest_data = np.delete(latest_data, np.array([batch_predict]), axis=1)\n",
    "    latest_data = np.append(latest_data, np.array([batch_predict]), axis=1)\n",
    "results = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(range(0, len(train_label)), train_label[:,0], color=(1.0, 0.7, 0.7), label='train_label temp')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,1], color=(0.7, 0.7, 1.0), label='train_label prec')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,2], color=(0.7, 1.0, 1.0), label='train_label humidity ')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,3], color=(1.0, 1.0, 0.7), label='train_label 25c_days')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,4], color=(0.7, 1.0, 1.0), label='train_label ice_sale')\n",
    "\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,0], color=(1.0, 0.0, 0.0), label='future temp')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,1], color=(0.0, 0.0, 1.0), label='future prec')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,2], color=(0.0, 1.0, 1.0), label='future humidity')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,3], color=(1.0, 1.0, 0.0), label='future 25c_days')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,4], color=(0.0, 1.0, 1.0), label='future ice_sales')\n",
    "\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 規則性のないbitcoinの値段を予測 (最終取引価格)\n",
    "\n",
    "規則性のあるデータは分析すれば機械学習を使わなくてもある程度予測は可能ですね。\n",
    "\n",
    "今度は規則性のない(見つけづらい)bitcoinの値段を使って学習してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3-1 データセットの作成\n",
    "\n",
    "データセットにbitflyerさんのpybitflyerを利用させていただきます。\n",
    "\n",
    "pybitflyerはアカウントいらずでデータを収集できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pybitflyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "対象のデータはbitcoinの日本円価格とします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api = pybitflyer.API()\n",
    "data = api.ticker(product_code='BTC_JPY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "様々なデータがありますが、この辺りが利用できそうですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data['ltp']) # 最終取引価格\n",
    "print(data['best_ask']) # 最高買い価格\n",
    "print(data['best_bid']) # 最小売り価格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "このデータを用途に応じて数分おき、数時間おきに取得します。\n",
    "\n",
    "5秒分のデータをとって見ましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "btc_jpy_data = []\n",
    "count = 5\n",
    "api = pybitflyer.API()\n",
    "while True:\n",
    "    tick = api.ticker(product_code='BTC_JPY')\n",
    "    print('tick={} ltp={}'.format(len(btc_jpy_data), tick['ltp']))\n",
    "    btc_jpy_data.append(tick)\n",
    "    time.sleep(1)\n",
    "    if count <= len(btc_jpy_data):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btc_jpy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "迷惑のかからない適度な利用にとどめたいですね。\n",
    "\n",
    "1000秒分のデータをjsonで保存してあるのでそれを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ファイルから\n",
    "import json\n",
    "with open('volume/datasets/BTC_JPY.json', 'r') as f:\n",
    "    btc_jpy_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "btc_jpy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "最終取引価格、最高買い価格、最低売り価格を利用してデータセットを作ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltp_data = np.array([[d['ltp']] for d in btc_jpy_data]) # 最終取引価格\n",
    "best_ask_data = np.array([[d['best_ask']] for d in btc_jpy_data]) # 最高買い価格\n",
    "best_bid_data = np.array([[d['best_bid']] for d in btc_jpy_data]) # 最小売り価格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以前同様、標準化/正規化します (すべての値を0~1の範囲に納める)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 標準化\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "ltp_data = scaler.fit_transform(ltp_data)\n",
    "best_ask_data = scaler.fit_transform(best_ask_data)\n",
    "best_bid_data = scaler.fit_transform(best_bid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "データの内容をみてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0, len(ltp_data)), ltp_data, color='r', label='ltp')\n",
    "plt.plot(range(0, len(best_ask_data)), best_ask_data, color='g', label='best_ask')\n",
    "plt.plot(range(0, len(best_bid_data)), best_bid_data, color='b', label='best_bid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5step(過去5秒を元に1秒後を予測)でデータを整えてみます。\n",
    "\n",
    "step数は自由に変えてもらって構いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltp_train_data, ltp_train_label = create_reccurent_dataset(ltp_data, step=5)\n",
    "best_ask_train_data, best_ask_train_label = create_reccurent_dataset(best_ask_data, step=5)\n",
    "best_bid_train_data, best_bid_train_label = create_reccurent_dataset(best_bid_data, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ltp_train_data.shape)\n",
    "print(best_ask_train_data.shape)\n",
    "print(best_bid_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ltp_train_label.shape)\n",
    "print(best_ask_train_label.shape)\n",
    "print(best_bid_train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "推論する際の形状 = (個数, ステップ数, 特徴量)\n",
    "\n",
    "今回の推論時の形状は入力も出力もltp, best_ask, best_bidの3つなので`(個数, 5, 3)`になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.c_[ltp_train_data, best_ask_train_data, best_bid_train_data]\n",
    "train_label = np.c_[ltp_train_label, best_ask_train_label, best_bid_train_label]\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3-2 モデル構築\n",
    "\n",
    "モデルを作成します。\n",
    "\n",
    "不安定な場合は学習率を上げ下げするといいでしょう。`Adam(lr=0.0001)`\n",
    "\n",
    "パラメータの調整はグリッドサーチなどパラメータ調整は色々ありますが、\n",
    "\n",
    "今回は使わないので低い学習率から3倍ずつ上げる方法が推奨されていました。 0.001 -> 0.003 -> 0.03 -> 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "step_count = 5\n",
    "feature_count = 3\n",
    "hidden_unit_count = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_unit_count, batch_input_shape=(None, step_count, feature_count), return_sequences=False))\n",
    "model.add(Dense(feature_count))\n",
    "model.add(Activation('linear'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='auto', patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3-3 トレーニング\n",
    "\n",
    "トレーニング実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# トレーニング\n",
    "model.fit(\n",
    "    train_data, train_label,\n",
    "    batch_size=2000,\n",
    "    epochs=200,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3-4 予測\n",
    "\n",
    "トレーニングデータと予測結果を検証してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,0], color=(1.0, 0.7, 0.7), label='train_label ltp')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,1], color=(0.7, 1.0, 0.7), label='train_label best_ask')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,2], color=(0.7, 0.7, 1.0), label='train_label best_bid')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,0], color=(1.0, 0.0, 0.0), label='predictions ltp')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,1], color=(0.0, 1.0, 0.0), label='predictions best_ask')\n",
    "plt.plot(range(0, len(predictions)), predictions[:,2], color=(0.0, 0.0, 1.0), label='predictions best_bid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "50秒後まで予測してみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latest_data = np.array([train_data[len(train_data)-1]])\n",
    "results = []\n",
    "for _ in range(50):\n",
    "    # 推論\n",
    "    batch_predict = model.predict(latest_data)\n",
    "    # 結果蓄積\n",
    "    results.append([batch_predict[0][0], batch_predict[0][1], batch_predict[0][2]])\n",
    "    # 次のデータをセット\n",
    "    latest_data = np.delete(latest_data, np.array([batch_predict]), axis=1)\n",
    "    latest_data = np.append(latest_data, np.array([batch_predict]), axis=1)\n",
    "results = np.array(results)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(0, len(train_label)), train_label[:,0], color=(1.0, 0.7, 0.7), label='train_label ltp')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,1], color=(0.7, 1.0, 0.7), label='train_label best_ask')\n",
    "plt.plot(range(0, len(train_label)), train_label[:,2], color=(0.7, 0.7, 1.0), label='train_label best_bid')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,0], color=(1.0, 0.0, 0.0), label='future ltp')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,1], color=(0.0, 1.0, 0.0), label='future best_ask')\n",
    "plt.plot(range(len(train_label)-1, len(train_label)-1+len(results)), results[:,2], color=(0.0, 0.0, 1.0), label='future best_bid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "トレーニング状況にもよりますが、試した際にはすこし上昇する予測がでました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## まとめ\n",
    "\n",
    "- リアルタイムにデータをとって学習させ、数日放置するのも良さそうです。\n",
    "- 単位がわかりにくいので特徴別に標準化する固定値を入れても良いかも\n",
    "- 特徴量を増やすと良いかもしれない 最大、最小、ローソク足の最大最小、関連ニュース数(あわよくば良し悪し別)、曜日\n",
    "- 予測できないイベント(プレスリリースが出た、ハッキングによって流出、政府による規制が発表されたなど)によって上下するので、後に出てくるデータではその不測は予測できないだろう\n",
    "- あくまでも傾向として参考程度にはなりそう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## おまけ word2vec\n",
    "\n",
    "\n",
    "単語を入力として、その周辺にどのような単語が現れやすいか予測するニューラルネットワーク(Skip-gram)があります。\n",
    "\n",
    "そのネットワークの中間層(隠れ層)の重みを単語ベクトルとして活用しよう！というのがword2vecです。\n",
    "\n",
    "単語ベクトルは座標です。 e.g. [0.74, 0.29, 0.98, 0.01, 0.13, 0.35]\n",
    "\n",
    "地図は緯度経度の2つが座標、高度が加わると3つの座標で高さもわかる、時間軸追加して...\n",
    "\n",
    "それ以降は人間にはイメージしにくいですが、その座標に近い単語だったり、座標の足し算引き算を行って単語の四則演算ができるようになります。\n",
    "\n",
    "[参考](https://deepage.net/bigdata/machine_learning/2016/09/02/word2vec_power_of_word_vector.html)\n",
    "\n",
    "まずはためしてみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### データセット\n",
    "\n",
    "まずは元となるテキストを用意します。\n",
    "\n",
    "[青空文庫](https://www.aozora.gr.jp/cards/001847/card57347.html)さん等からテキストデータをダウンロードしてきましょう。\n",
    "\n",
    "slackの会話データとってきたりしても面白そう。\n",
    "\n",
    "テキストの中を確認して必要な文だけに絞ってください。\n",
    "\n",
    "`volume/datasets`などに配置して、適宜pathを書き換えてください。\n",
    "\n",
    "corrected_text_pathは加工したテキストを保存するpathです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_path = 'volume/datasets/book.txt'\n",
    "corrected_text_path = 'volume/datasets/book_corrected.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "データはmecabで形態素解析してわかちがきした(基本形に統一した)テキストを用意します。\n",
    "\n",
    "具体的には下記のようになります\n",
    "\n",
    "'彼は老いていた' ->'彼,は,老いる,て,いる,た '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dockerfileにinstall mecab-ipadicと書かれてますが、新しい日本語辞書(と言っても2016-05-02ですが)を含めて、最近の言葉に対応できるようになっています。\n",
    "\n",
    "サービス独自の単語がある場合は辞書を作ってみるのもいいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "適宜使用するテキストの文字コードに応じて書き換えましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(text_path, 'r', 'shift-jis') as file:\n",
    "    texts = file.read()\n",
    "lines = texts.split(\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "空白やハイフンを除去して整形したテキストファイルを作ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Owakati')\n",
    "\n",
    "with codecs.open(corrected_text_path, 'w') as corrected_file:\n",
    "    for line in lines:\n",
    "        if not line:\n",
    "            # 空行を除く処理\n",
    "            continue\n",
    "        if line[0] == \"-\":\n",
    "            # ハイフンの処理\n",
    "            continue\n",
    "        corrected_file.write(tagger.parse(line))\n",
    "\n",
    "\n",
    "with codecs.open(corrected_text_path, 'r') as corrected_file:\n",
    "    doc = corrected_file.read()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "整形したテキストをword2vecで学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "sentences = word2vec.LineSentence(corrected_text_path)\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences, \n",
    "    sg=1, # 1=skip-gram, 0=C-BOW\n",
    "    size=100, # 単語の次元数\n",
    "    min_count=1, # n回未満登場する単語を破棄 頻出のみ学習したいのなら上げる\n",
    "    window=10, # 周囲10個の単語に対して\n",
    "    hs=1, # 階層化ソフトマックス使用フラグ\n",
    "    negative=0 # ネガティブサンプリングに用いる単語数 ドロップアウトのようなもの\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "類似語と類似度をしらべるにはmost_similarメソッドを利用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['人'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "テキストが少ないのでそこまで良い結果にはなってないはずです。\n",
    "\n",
    "もっと大量にテキストデータとトレーニングする時間が必要なのですが、wikiのテキストを学習したモデルが公開されていましたのでそれを利用させていただきます！\n",
    "\n",
    "[word2vecの学習済み日本語モデルを公開します 白ヤギコーポレーション](http://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('volume/models/latest-ja-word2vec-gensim-model/word2vec.gensim.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "単語ベクトルの数とその数値をみてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv['ピザ'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv['ピザ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "類似語と類似度をしらべるにはmost_similarメソッドを利用します\n",
    "\n",
    "positiveが近い単語、negativeが遠い単語です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['ピザ']) # or model.similar_by_word('ピザ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(negative=['ピザ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "単語と単語間の類似度、距離を調べるにはsimilarityメソッドを使います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 類似度\n",
    "print(model.wv.similarity('ピザ', 'ハンバーガー'))\n",
    "print(model.wv.similarity('ピザ', 'パン'))\n",
    "print(model.wv.similarity('ピザ', 'スプーン'))\n",
    "print(model.wv.similarity('ピザ', 'イス'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "similar_by_vectorメソッドで、単語ではなくベクトル値から近い単語を出力できるので、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ベクトルから類似語と類似度\n",
    "vector = model.wv['ピザ']\n",
    "model.wv.similar_by_vector(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ベクトル値を計算した後の近い単語を出力できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector = model.wv['王様'] - model.wv['じいさん'] + model.wv['女']\n",
    "model.wv.similar_by_vector(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector = model.wv['イチロー'] - model.wv['野球'] + model.wv['サッカー']\n",
    "model.wv.similar_by_vector(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector = model.wv['宇宙人'] - model.wv['人']\n",
    "model.wv.similar_by_vector(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 何に利用できるか\n",
    "\n",
    "- 単語から類似度が取れるので関連商品を表示するなどレコメンと\n",
    "- レコメンドと似たようなものだが、関連性の高い質問と紐づけるQ&Aへの利用\n",
    "- 他にも機械翻訳や感情分析にも利用されている事例がある\n",
    "\n",
    "### 弱点\n",
    "\n",
    "単語の周囲の文字から単語ベクトルを学習するので、対義語を出すことができません。\n",
    "\n",
    "具体的には、『あなた が 好き です』『あなた が 嫌い です』のような文にたいして、\n",
    "\n",
    "`が`の次が`好き`や`嫌い`だと学習し、\n",
    "\n",
    "`です`の前が`好き`や`嫌い`だと学習するため、\n",
    "\n",
    "`好き`と`嫌い`のベクトルは近くなってしまいます。\n",
    "\n",
    "実際 `ピザ`からかけ離れたベクトルは`CNT`や`基き`になり、全くねじれた方向の単語になります。\n",
    "\n",
    "ただ、`インストール`に近いベクトルに`アンインストール`などの対義語が含まれるので、\n",
    "\n",
    "『インストール方法』を検索した人に対して『アンインストール方法』をレコメンドするなど、この特性はレコメンドとしては良い弱点になりそうな気がします。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
